# 2016 1124 
今天主要看了一点关于数据库的相关知识，下面说一点自己的理解
>为什么要看数据库

数据库是数据持久化存储的一种有效方式而且支持有序查询，web开发中根本离不开数据库

>python 数据库访问

pyton 访问数据库主要有两种方式

1. 通过DB-API直接执行SQL语句
2. 通过ORM 操作python对象实现一种映射进而操作数据库

>书籍对比

核心编程侧重讲了如何运用已有的api来实现对数据库的操作，主要讲运用
面向对象编程指南则对数据库访问的实现原理进行了一些讲解

> 明天目标

继续看面向对象编程指南这本书中的第11章，最终的目的是明白python 操作数据库的原理
检验手段

1. 自己可以封装底层的SQL操作
2. 读懂廖雪峰python教程中关于DB模块的设计

###小目标之自建网站需求分析
##2016 1129
11月也快过去了，目标还是不明确，还在挥霍时间，珍惜啊

今天主要理清了SQL语言的一些特性.

  - SQL 语言是一个声明，输出的是一个检索集合
  - SQL语言是有执行顺序的FROM，WHERE
  - SQL关键是对表的应用，将不同的表链接起来最终输出你想要的结果 

===============
未来两周时间会写出一个简化板的知乎出来，整体的后端建构参照flaskweb开发这本书来设计，首先列出开发计划：

- 解构知乎PC端的页面，先实现部分模块，如首页内容推荐
- 确认所需逻辑模块，分几个功能模块
- 确定需要的数据，构建数据库模型
- 确认好整体的代码结构与框架
- 编写代码

> 目前只是初步的做了一个计划，后续会给出花在每个部分的时间，页面部分在考虑使用flask-bootstrap 与 react 集合来设计，趁此机会也了解一下react

#明早7点开工！！！
#厉害了我的哥，明早7点起床看你是否活着......

#20161130
###网站目的
整个网站是围绕着书籍展开的，展示每个用户关注的书籍分类，然后用户可以对每本书籍进行点赞与评论。
界面风格模仿 知乎+豆瓣 采用知乎的框架 + 豆瓣的书籍展现方式 采用单页面网页形式

===========================
###网站架构
> server 端 flask
> 页面端 flask-bootsrap + react
> 数据库 采用 mysql 或者 sqlite


===========================

##页面结构
> 导航栏结构

  - 搜索框（具体可以指定搜索某本书）
  - 首页（展示用户关注的书籍）
      -  子菜单书籍分类 编程 文学 国学 科技 等
  - 话题（保留）
  - 发现（保留）
  - 消息（保留）
  - 用户信息（展示用户的信息，设计成可持续添加模块）
      - 头像
      - 位置
      - 邮箱
  
> 主页面结构

- 书籍列表
  - 书名 + 作者 + 出版社 + 简介 。。。
  - 评论（可折叠）
  - 点赞
      
===========================
 > 下一步计划

  1. 写爬虫爬取豆瓣的一些书籍（预计时间1-2天 12月2号前结束）
      - 解析豆瓣书的html页面找出规律
      - 设计书籍的数据库模型model 类
      - 将怕入的书籍存入数据库中
  2. 分析前端 与 后端 需要交互的数据以及设计数据库连接表（2天 12月5号前）
  3.  划分后端逻辑模块 （1天 12月6号前）
  4.  依据前端划分的结构写前端代码 （5天  12月10号）
      - 学习react 
  5.  写后端代码 （5天 12月15号）
  6.  整合前后端代码调试（1天 12月16号）

#计划可以改变但是时间点不变12月16号出产品

===============================

#1202
已经12月了！

因为想从豆瓣上爬一些书籍资料，所以去学习了scrapy，以下是一些心得
首先scrapy目前支持python3 但是对windows下的python3 支持不友好，需要最新的vc++ 2015 这个我都没有装成功，mac ，linux 上就支持的比较好。然后就是要先了解一下xpath

> xpath

xpath是一个基于html节点，元素等进行定位解析的一个语法，与正则表达式不同的是，依据dom节点来操作，而不是匹配字符串。然后就是chrome支持copy出xpath可以节省不少时间
> scrapy

scrapy 是一个 强大的爬虫框架，提供一整套的解决方案，爬取，解析，处理，同时有很好的做了模块化，具体代码模块参照turorial代码

目前抓取单页面的应用，已经ok，但是依据解析到的url递归抓取，还未实现，然后就是数据处理pipelines部分还没有怎么了解，这是明天要搞定的内容，明天要完成豆瓣图书编程书籍100本的爬取

豆瓣的反爬虫机制目前我只会做到模拟浏览器登录user-agent，还不知道能坚持爬几次，所以目前采用的方法是先将网页源码下载下来，然后通过lxml库去解析，然后再用怕虫去爬
